{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce8ce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.8203 - loss: 0.4263 - val_accuracy: 0.9776 - val_loss: 0.0908\n",
      "Epoch 2/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.9798 - loss: 0.0745 - val_accuracy: 0.9843 - val_loss: 0.0614\n",
      "Epoch 3/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9903 - loss: 0.0468 - val_accuracy: 0.9854 - val_loss: 0.0514\n",
      "Epoch 4/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.9951 - loss: 0.0228 - val_accuracy: 0.9854 - val_loss: 0.0499\n",
      "Epoch 5/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9949 - loss: 0.0154 - val_accuracy: 0.9865 - val_loss: 0.0517\n",
      "Epoch 6/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.9993 - loss: 0.0048 - val_accuracy: 0.9865 - val_loss: 0.0590\n",
      "Epoch 7/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.9998 - loss: 0.0022 - val_accuracy: 0.9843 - val_loss: 0.0645\n",
      "Epoch 8/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.9926 - loss: 0.0187 - val_accuracy: 0.9832 - val_loss: 0.0655\n",
      "Epoch 9/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.9999 - loss: 0.0020 - val_accuracy: 0.9832 - val_loss: 0.0622\n",
      "Epoch 10/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.9865 - val_loss: 0.0689\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9845 - loss: 0.0670\n",
      "Test Accuracy: 0.9812\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "Actual: 0, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\ASUS\\\\Desktop\\\\capstone vickey\\\\spam.csv\", encoding='latin1')\n",
    "\n",
    "# Check the columns\n",
    "print(data.columns)\n",
    "\n",
    "# Assume the columns are 'v1' for labels and 'v2' for text\n",
    "data = data[['v1', 'v2']]\n",
    "data.columns = ['Label', 'Text']\n",
    "\n",
    "# Encode the labels\n",
    "encoder = LabelEncoder()\n",
    "data['Label'] = encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 5000\n",
    "max_sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['Text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['Text'])\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "y = data['Label'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_sequence_length))\n",
    "model.add(SimpleRNN(64, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Print some example predictions\n",
    "for i in range(10):\n",
    "    print(f'Actual: {y_test[i]}, Predicted: {predictions[i][0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ca05de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "New Dataset Accuracy: 0.6410\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 1\n",
      "Actual: 0, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 1\n",
      "Actual: 0, Predicted: 1\n",
      "Actual: 0, Predicted: 1\n",
      "Actual: 0, Predicted: 1\n",
      "Actual: 0, Predicted: 1\n",
      "Actual: 0, Predicted: 1\n"
     ]
    }
   ],
   "source": [
    "# Function to predict on new data\n",
    "def predict_new_data(new_data, model, tokenizer, max_sequence_length):\n",
    "    # Convert all entries in the 'Text' column to strings\n",
    "    new_data['Text'] = new_data['Text'].astype(str)\n",
    "    \n",
    "    # Tokenize and pad the new data\n",
    "    new_sequences = tokenizer.texts_to_sequences(new_data['Text'])\n",
    "    new_X = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Make predictions\n",
    "    new_predictions = model.predict(new_X)\n",
    "    new_predictions = (new_predictions > 0.5).astype(int)\n",
    "    \n",
    "    return new_predictions\n",
    "\n",
    "# Function to calculate accuracy on new data\n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    correct = sum(true_labels == predicted_labels)\n",
    "    total = len(true_labels)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Example usage for new dataset\n",
    "# Assuming you have a new dataset in the same format as the original\n",
    "new_data = pd.read_csv(\"C:\\\\Users\\\\ASUS\\\\Desktop\\\\capstone vickey\\\\new dataset of spam deduction\\\\spam_or_not_spam.csv\", encoding='latin1')\n",
    "new_data = new_data[['v1', 'v2']]\n",
    "new_data.columns = ['Label', 'Text']\n",
    "\n",
    "# Remove rows with NaN values\n",
    "new_data = new_data.dropna()\n",
    "\n",
    "# Encode the labels for the new dataset\n",
    "new_data['Label'] = encoder.transform(new_data['Label'])\n",
    "\n",
    "# Make predictions on the new dataset\n",
    "new_predictions = predict_new_data(new_data, model, tokenizer, max_sequence_length)\n",
    "\n",
    "# Calculate accuracy on the new dataset\n",
    "new_accuracy = calculate_accuracy(new_data['Label'].values, new_predictions.flatten())\n",
    "print(f'New Dataset Accuracy: {new_accuracy:.4f}')\n",
    "\n",
    "# Print some example predictions from the new dataset\n",
    "for i in range(min(10, len(new_data))):\n",
    "    print(f'Actual: {new_data[\"Label\"].values[i]}, Predicted: {new_predictions[i][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcebd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step\n",
      "Text: Congratulations! You've won a free iPhone! Click here to claim your prize.\n",
      "Prediction: Spam\n",
      "\n",
      "Text: Hi Mom, can you pick me up after school today?\n",
      "Prediction: Not Spam\n",
      "\n",
      "Text: URGENT: Your bank account has been compromised. Reply with your details immediately.\n",
      "Prediction: Not Spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save('spam_model.h5')\n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Function to preprocess and predict on new data\n",
    "def predict_spam(new_texts):\n",
    "    # Load the saved model and tokenizer\n",
    "    loaded_model = load_model('spam_model.h5')\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        loaded_tokenizer = pickle.load(handle)\n",
    "    \n",
    "    # Tokenize and pad the new texts\n",
    "    new_sequences = loaded_tokenizer.texts_to_sequences(new_texts)\n",
    "    new_X = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Make predictions\n",
    "    new_predictions = loaded_model.predict(new_X)\n",
    "    new_predictions = (new_predictions > 0.5).astype(int)\n",
    "    \n",
    "    return new_predictions\n",
    "\n",
    "# Example usage of the prediction function\n",
    "new_texts = [\n",
    "    \"Congratulations! You've won a free iPhone! Click here to claim your prize.\",\n",
    "    \"Hi Mom, can you pick me up after school today?\",\n",
    "    \"URGENT: Your bank account has been compromised. Reply with your details immediately.\"\n",
    "]\n",
    "\n",
    "results = predict_spam(new_texts)\n",
    "\n",
    "for text, result in zip(new_texts, results):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {'Spam' if result[0] == 1 else 'Not Spam'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab205d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c07c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76dc5690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 74ms/step - accuracy: 0.8546 - loss: 0.4141 - val_accuracy: 0.9809 - val_loss: 0.0721\n",
      "Epoch 2/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 72ms/step - accuracy: 0.9857 - loss: 0.0517 - val_accuracy: 0.9843 - val_loss: 0.0604\n",
      "Epoch 3/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 69ms/step - accuracy: 0.9960 - loss: 0.0167 - val_accuracy: 0.9843 - val_loss: 0.0638\n",
      "Epoch 4/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 0.9843 - val_loss: 0.0748\n",
      "Epoch 5/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9865 - val_loss: 0.0665\n",
      "Epoch 6/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9999 - loss: 6.6700e-04 - val_accuracy: 0.9854 - val_loss: 0.0654\n",
      "Epoch 7/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 4.4421e-04 - val_accuracy: 0.9865 - val_loss: 0.0719\n",
      "Epoch 8/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 2.1924e-04 - val_accuracy: 0.9865 - val_loss: 0.0813\n",
      "Epoch 9/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 1.4132e-04 - val_accuracy: 0.9877 - val_loss: 0.0831\n",
      "Epoch 10/10\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 1.1335e-04 - val_accuracy: 0.9865 - val_loss: 0.0871\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9873 - loss: 0.0711\n",
      "Test Accuracy: 0.9848\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 0, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 1, Predicted: 1\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "Actual: 0, Predicted: 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step\n",
      "Text: Congratulations! You've won a free ticket to Bahamas!\n",
      "Prediction: Not Spam\n",
      "\n",
      "Text: Can we reschedule our meeting to next week?\n",
      "Prediction: Not Spam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "import pickle\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\ASUS\\\\Desktop\\\\capstone vickey\\\\spam.csv\", encoding='latin1')\n",
    "\n",
    "# Check the columns\n",
    "print(data.columns)\n",
    "\n",
    "# Assume the columns are 'v1' for labels and 'v2' for text\n",
    "data = data[['v1', 'v2']]\n",
    "data.columns = ['Label', 'Text']\n",
    "\n",
    "# Encode the labels\n",
    "encoder = LabelEncoder()\n",
    "data['Label'] = encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_words = 5000\n",
    "max_sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['Text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['Text'])\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "y = data['Label'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_sequence_length))\n",
    "model.add(GRU(64, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Print some example predictions\n",
    "for i in range(10):\n",
    "    print(f'Actual: {y_test[i]}, Predicted: {predictions[i][0]}')\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save('spam_gru_model.h5')\n",
    "with open('spam_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Function to preprocess and predict on new data\n",
    "def predict_spam(new_texts):\n",
    "    # Load the saved model and tokenizer\n",
    "    loaded_model = load_model('spam_gru_model.h5')\n",
    "    with open('spam_tokenizer.pickle', 'rb') as handle:\n",
    "        loaded_tokenizer = pickle.load(handle)\n",
    "    \n",
    "    # Tokenize the new texts\n",
    "    sequences = loaded_tokenizer.texts_to_sequences(new_texts)\n",
    "    max_sequence_length = 100\n",
    "    X_new = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Predict using the loaded model\n",
    "    predictions = loaded_model.predict(X_new)\n",
    "    predictions = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "new_texts = [\"Congratulations! You've won a free ticket to Bahamas!\", \"Can we reschedule our meeting to next week?\"]\n",
    "predictions = predict_spam(new_texts)\n",
    "\n",
    "for text, prediction in zip(new_texts, predictions):\n",
    "    print(f'Text: {text}\\nPrediction: {\"Spam\" if prediction[0] == 1 else \"Not Spam\"}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cce36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
